{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITCS 6010: Assignment #1 (v3)  \n",
    "\n",
    "<font color=\"red\">(Due: 11 pm on Sep 26th) </font>\n",
    "\n",
    "**Submitted by : Aaroh Mathur (801076696)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Softmax] Show that in the case of two actions, the softmax operation using the Gibbs distribution becomes the logistic, or sigmoid, function commonly used in artificial neural networks. What effect does the temperature parameter have on the function? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets see the defination of Gibbs Distribution.\n",
    "\n",
    "**Gibbs Distribution (also known as Boltzmann distribution)**\n",
    "\n",
    "This ditribution is a probability distribution or probability measure that gives the probability that a system will be in a certain state as a function of that state's energy and the temperature of the system.\n",
    "\n",
    "$$p_{i} \\propto e^{\\frac{-\\varepsilon_{i}}{kT}}$$\n",
    "\n",
    "where $p_{i}$ is the probability of the system being in state $i$, $\\varepsilon_{i}$ is the energy of that state, and a constant $kT$ of the distribution is the product of Boltzmann's constant $k$ and thermodynamic temperature $T$. The symbol $\\propto$ denotes proportionality.\n",
    "\n",
    "The Distribution can be formulated as:\n",
    "\n",
    "$$ p_{i}=\\frac{1}{Q}e^{\\frac{-\\varepsilon_{i}}{kT}}=\\frac{e^{\\frac{-\\varepsilon_{i}}{kT}}}{\\sum_{j=1}^{M} {e^{\\frac{-\\varepsilon_{j}}{kT}}}}$$\n",
    "\n",
    "Lets assume the two actions $ A={a, b} $. Given these two actions the probabilities calculated are:\n",
    "$$\n",
    "\\frac{e^{a / \\tau}}{e^{a / \\tau}+e^{b / \\tau}} \\quad \\text { and } \\quad \\frac{e^{b / \\tau}}{e^{a / \\tau}+e^{b / \\tau}}\n",
    "$$\n",
    "\n",
    "Multiplying $ {e^{-a / \\tau}} $ with the first expression's numerator and denominator, We get\n",
    "$$\n",
    "\\frac{1}{1+e^{(b-a) / \\tau}}\n",
    "$$\n",
    "\n",
    "which forms the defination of softmax function. The temperature $\\tau$ here plays the role of how quickly the incoming signal is “squashed”. The larger $\\tau$ is the more quickly the function squashes its inputs and the distribution becomes uniform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. If the step-size parameters, $\\alpha_k$, are not constant, then the estimate $Q_k$ is a wighted average of previously received rewards with a weighting different from that given by (2.6) in p32. What is the weighting on each prior reward for the general case?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increamental update rule for updating an average $Q_{k}$ of the $k-1$ past rewards is modified to be\n",
    "\n",
    "$\n",
    "Q_{k} \\doteq Q_{k-1}+\\alpha\\left[R_{k-1}-Q_{k-1}\\right]\n",
    "$\n",
    "\n",
    "Now from this equation we can calculate the weighted average of past rewards for the general case where step-size parameter, $\\alpha_k$ are not constant\n",
    "\n",
    "$\n",
    "$\\begin{aligned} Q_{k} &=Q_{k-1}+\\alpha_{k}\\left(r_{k}-Q_{k-1}\\right) \\\\ &=\\alpha_{k} r_{k}+\\left(1-\\alpha_{k}\\right) Q_{k-1} \\\\ &=\\alpha_{k} r_{k}+\\left(1-\\alpha_{k}\\right)\\left[\\alpha_{k-1} r_{k-1}+\\left(1-\\alpha_{k-1}\\right) Q_{k-2}\\right] \\\\ &=\\alpha_{k} r_{k}+\\left(1-\\alpha_{k}\\right) \\alpha_{k-1} r_{k-1}+\\left(1-\\alpha_{k}\\right)\\left(1-\\alpha_{k-1}\\right) Q_{k-2} \\\\ &=\\alpha_{k} r_{k}+\\left(1-\\alpha_{k}\\right) \\alpha_{k-1} r_{k-1}+\\left(1-\\alpha_{k}\\right)\\left(1-\\alpha_{k-1}\\right)\\left[\\alpha_{k-2} r_{k-2}+\\left(1-\\alpha_{k-2}\\right) Q_{k-3}\\right] \\\\ &=\\alpha_{k} r_{k}+\\left(1-\\alpha_{k}\\right) \\alpha_{k-1} r_{k-1}+\\left(1-\\alpha_{k}\\right)\\left(1-\\alpha_{k-1}\\right) \\alpha_{k-2} r_{k-2} \\\\ &+\\left(1-\\alpha_{k}\\right)\\left(1-\\alpha_{k-1}\\right)\\left(1-\\alpha_{k-2}\\right) Q_{k-3} \\\\ &= \\cdots \\\\ &=\\alpha_{k} r_{k}+\\left(1-\\alpha_{k}\\right) \\alpha_{k-1} r_{k-1}+\\left(1-\\alpha_{k}\\right)\\left(1-\\alpha_{k-1}\\right) \\alpha_{k-2} r_{k-2}+\\left(1-\\alpha_{k}\\right)\\left(1-\\alpha_{k-1}\\right)\\left(1-\\alpha_{k-2}\\right) \\cdots\\left(1-\\alpha_{3}\\right) \\alpha_{2} r_{2}+\\left(1-\\alpha_{k}\\right)\\left(1-\\alpha_{k-1}\\right)\\left(1-\\alpha_{k-2}\\right) \\cdots\\left(1-\\alpha_{2}\\right) \\alpha_{1} r_{1}+\\left(1-\\alpha_{k}\\right)\\left(1-\\alpha_{k-1}\\right)\\left(1-\\alpha_{k-2}\\right) \\cdots\\left(1-\\alpha_{2}\\right)\\left(1-\\alpha_{1}\\right) Q_{0} \\end{aligned}$\n",
    "$\n",
    "\n",
    "$\n",
    "\\text { Thus in the general case the weight on the prior reward } Q_{0} \\text { is given by } \\prod_{i=1}^{k}\\left(1-\\alpha_{i}\\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the Bellman equation for action values, for $Q^\\pi$? It must give the action value $Q^\\pi(s,a)$ in terms of the action values, $Q^\\pi(s^\\prime, a^\\prime)$, of possible successors to the state-action pair $(s,a)$. Show the sequence of equations analogous to (3.14), but for action values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets derive the Bellman equation in terms of $Q^\\pi$ considering the state-action pair $(s, a)$\n",
    "\n",
    "We know that,\n",
    "$$\n",
    "\\begin{aligned} Q^{\\pi}(s, a) &=E_{\\pi}\\left\\{R_{t} | s_{t}=s, a_{t}=a\\right\\} \\\\ &=E_{\\pi}\\left\\{\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} | s_{t}=s, a_{t}=a\\right\\} \\end{aligned}\n",
    "$$\n",
    "\n",
    "Lets include the $s^\\prime$ which the environment puts the agent into once action $a$ is taken on state $s$\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a)=\\sum_{s^{\\prime}} E_{\\pi}\\left\\{\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} | s_{t}=s, a_{t}=a, s_{t+1}=s^{\\prime}\\right\\} P\\left\\{s_{t+1}=s^{\\prime} | s_{t}=s, a_{t}=a\\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{array}{l}{\\text {Changing notation by introducing } P_{s, s^{\\prime}}^{a} \\text { as the second probability factor above, releasing the }}\\\\ {k=0 \\text { term, and factoring out a } \\gamma \\text { in the second summation we find that we now have } Q^{\\pi}(s, a)} \\\\ {\\text { given by }}\\end{array}\n",
    "$\n",
    "\n",
    "$$\n",
    "\\begin{aligned} Q^{\\pi}(s, a) &=\\sum_{s^{\\prime}} E_{\\pi}\\left\\{r_{t+1} | s_{t}=s, a_{t}=a, s_{t+1}=s^{\\prime}\\right\\} P_{s, s^{\\prime}}^{a} \\\\ &+\\gamma \\sum_{s^{\\prime}} E_{\\pi}\\left\\{\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+2} | s_{t}=s, a_{t}=a, s_{t+1}=s^{\\prime}\\right\\} P_{s, s^{\\prime}}^{a} \\end{aligned}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\begin{array}{l}{\\text {Introducing } R_{s, s^{\\prime}}^{a} \\text { in the first expectation above and the fact that we assuming Markov }} {\\text {states the above becomes }} \\end{array}\n",
    "$\n",
    "$$\n",
    "{\\qquad Q^{\\pi}(s, a)=\\sum_{s^{\\prime}} R_{s, s^{\\prime}}^{a} P_{s, s^{\\prime}}^{a}+\\gamma \\sum_{s^{\\prime}} E_{\\pi}\\left\\{\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+2} | s_{t+1}=s^{\\prime}\\right\\} P_{s, s^{\\prime}}^{a}}\n",
    "$$\n",
    "\n",
    "We now use the conditional expectation theorem again this time on the expectation termconditioning on the possible actions we can take from state $s^\\prime$. We have\n",
    "\n",
    "$$\n",
    "\\begin{aligned} E_{\\pi}\\left\\{\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+2} | s_{t+1}=s^{\\prime}\\right\\} &=E_{\\pi}\\left\\{R_{t+1} | s_{t+1}=s^{\\prime}\\right\\} \\\\ &=\\sum_{a^{\\prime}} E_{\\pi}\\left\\{R_{t+1} | s_{t+1}=s^{\\prime}, a_{t+1}=a^{\\prime}\\right\\} P\\left\\{a_{t+1}=a^{\\prime} | s_{t+1}=s^{\\prime}\\right\\} \\\\ &=\\sum_{a^{\\prime}} E_{\\pi}\\left\\{R_{t+1} | s_{t+1}=s^{\\prime}, a_{t+1}=a^{\\prime}\\right\\} \\pi\\left(s^{\\prime}, a^{\\prime}\\right) \\\\ &=\\sum_{a^{\\prime}} Q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\pi\\left(s^{\\prime}, a^{\\prime}\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\begin{array}{l}{\\text { When this is substituted into the expression for } Q^{\\pi}(s, a) \\text { we obtain }}\\end{array}\n",
    "$\n",
    "$$\n",
    "{\\qquad Q^{\\pi}(s, a)=\\sum_{s^{\\prime}} R_{s, s^{\\prime}}^{a} P_{s, s^{\\prime}}^{a}+\\gamma \\sum_{s^{\\prime}} \\sum_{a^{\\prime}} Q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\pi\\left(s^{\\prime}, a^{\\prime}\\right) P_{s, s^{\\prime}}^{a}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Bellman equation (3.4) must hold for each state for the value function $V^\\pi$ showin Figure 3.2 (p60). Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, -0.4, and +0.7. (These numbers are accurate only to one decimal place.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We want to check that at the center state we have equality in Bellman's equation. Since  the left hand side is given by $V^{\\pi}(s)=+0.7$ we check that the right hand side is equal to this. Letting the letters $u, d, l,$ and $r$ denote the squares to the up, down, left, and right respectively we see that the right hand side of Bellman's equation is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned} & \\pi(s, a=u) \\sum_{s^{\\prime}} P_{s, s^{\\prime}}^{a=u}\\left(R_{s, s^{\\prime}}^{a=u}+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right)+\\pi(s, a=r) \\sum_{s^{\\prime}} P_{s, s^{\\prime}}^{a=r}\\left(R_{s, s^{\\prime}}^{a=r}+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right) \\\\+& \\pi(s, a=d) \\sum_{s^{\\prime}} P_{s, s^{\\prime}}^{a=d}\\left(R_{s, s^{\\prime}}^{a=d}+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right)+\\pi(s, a=l) \\sum_{s^{\\prime}} P_{s, s^{\\prime}}^{a=l}\\left(R_{s, s^{\\prime}}^{a=l}+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right) \\\\=& \\frac{1}{4}\\left(R_{s, s^{\\prime}=u}^{a=u}+\\gamma V^{\\pi}\\left(s^{\\prime}=u\\right)\\right)+\\frac{1}{4}\\left(R_{s, s^{\\prime}=r}^{a=r}+\\gamma V^{\\pi}\\left(s^{\\prime}=r\\right)\\right) \\\\+& \\frac{1}{4}\\left(R_{s, s^{\\prime}=d}^{a=d}+\\gamma V^{\\pi}\\left(s^{\\prime}=d\\right)\\right)+\\frac{1}{4}\\left(R_{s, s^{\\prime}=l}^{a}+\\gamma V^{\\pi}\\left(s^{\\prime}=l\\right)\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "This is because $R=0$ for all of the these steps the above becomes\n",
    "$$\n",
    "\\begin{aligned} & \\frac{\\gamma}{4}\\left(V^{\\pi}\\left(s^{\\prime}=u\\right)+V^{\\pi}\\left(s^{\\prime}=r\\right)+V^{\\pi}\\left(s^{\\prime}=d\\right)+V^{\\pi}\\left(s^{\\prime}=l\\right)\\right) \\\\=& \\frac{0.9}{4}(2.3+0.4-0.4+0.7)=\\frac{2.7}{4} \\approx 0.7 \\end{aligned}\n",
    "$$\n",
    "\n",
    "to the accuracy of the numbers given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. In Example 4.1 (p76), suppose a new state 15 is added to the gridworld just below state 13, and its actions, *left*, *up*, *right*, and *down*, take the agent  12,13,14, and 15, respecively. Assume that the transitions from the original states ae unchanged. What, then, is $V^\\pi(15)$ for the equiprobable random policy? \n",
    "\n",
    "Given the assumption, it means that the state-value function values do not change for the original grid locations when this newly added state is appended.\n",
    "\n",
    "Therefore now we can use the Equation 4.5 to obtain the state value function for this new state $V^\\pi(15)$. We find that\n",
    "\n",
    "Action set ${l, u, r, d}$ as left, up, right and down\n",
    "$$\n",
    "\\begin{aligned} V^{\\pi}(15) &=\\sum_{a} \\pi(15, a) \\sum_{s^{\\prime}} P_{15, s^{\\prime}}^{a}\\left(R_{15, s^{\\prime}}^{a}+\\gamma V\\left(s^{\\prime}\\right)\\right) \\\\ &=\\frac{1}{4}\\left(R_{15,12}^{\\mathrm{l}}+\\gamma V(12)+R_{15,13}^{\\mathrm{u}}+\\gamma V(13)\\right.\\\\ &\\left.+R_{15,14}^{\\mathrm{r}}+\\gamma V(14)+R_{15,15}^{\\mathrm{d}}+\\gamma V(15)\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "Subsituting values of $V^\\pi$ as given by Figure 4.2\n",
    "\n",
    "$$\n",
    "\\begin{aligned} V^{\\pi}(15) &=\\frac{1}{4}(-1-22 \\gamma-1-20 \\gamma-1-\\gamma 14-1+\\gamma V(15)) \\\\ &=-1-14 \\gamma+\\frac{\\gamma}{4} V(15) \\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally after solving for $V(15)$ we get \n",
    "\n",
    "$$\n",
    "V(15)=-\\frac{4(1+14 \\gamma)}{4-\\gamma}\n",
    "$$\n",
    "\n",
    "If we take $\\gamma = 0.99$ then we find $V(15) = -19.74$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Derive the weighted-average update rule (5.8) from (5.7) (p109). Follow the pattern of the derivation of the unweighted rule (2.3).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Equation \n",
    "$$\n",
    "V_{n}=\\frac{\\sum_{k=1}^{n} w_{k} R_{k}}{\\sum_{k=1}^{n} w_{k}}\n",
    "$$\n",
    "\n",
    "Recursive formula can be derived for $V_n$ by incrementing $n$ by one to get $V_{n+1}$ and separating the last $w_{n+1}$\n",
    "\n",
    "$$\n",
    "V_{n+1}=\\frac{\\sum_{k=1}^{n+1} w_{k} R_{k}}{\\sum_{k=1}^{n+1} w_{k}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\frac{\\sum_{k=1}^{n} w_{k}}{\\sum_{k=1}^{n+1} w_{k}}\\left(\\frac{\\sum_{k=1}^{n} w_{k} R_{k}}{\\sum_{k=1}^{n} w_{k}}\\right)+\\left(\\frac{w_{n+1}}{\\sum_{k=1}^{n+1} w_{k}}\\right) R_{n+1}\n",
    "$$\n",
    "\n",
    "Let $W_n = \\sum_{k=1}^{n}{w_k}$, we can rewrite the above equation as\n",
    "$$\n",
    "V_{n+1}=\\frac{W_{n}}{W_{n+1}} V_{n}+\\frac{w_{n+1}}{W_{n+1}} R_{n+1}\n",
    "$$\n",
    "\n",
    "Now $W_n$ can be written as $W_{n+1} - w_{n+1}$, The fraction $\\frac{W_n}{W_{n+1}}$ becomes $1 - \\frac{w_{n+1}}{W_{n+1}}$.\n",
    "\n",
    "Now $V_{n+1}$ can be represented as\n",
    "\n",
    "$$\n",
    "\\begin{aligned} V_{n+1} &=V_{n}-\\frac{w_{n+1}}{W_{n+1}} V_{n}+\\frac{w_{n+1}}{W_{n+1}} R_{n+1} \\\\ &=V_{n}-\\frac{w_{n+1}}{W_{n+1}}\\left(R_{n+1}-V_{n}\\right) \\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. \n",
    "###  1) Modify the algorithm for first-visit MC policy evaluation (Figure 5.1)  to use the incremental implementation for stationary averages. <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{l}{\\text { Input: a policy } \\pi \\text { to be evaluated }} \\\\ {\\text { Initialize: }} \\\\ {\\qquad V(s) \\in \\mathbb{R}, \\text { arbitrarily, for all } s \\in \\mathcal{S}} \\\\ {\\qquad \\begin{array}{l} {\\text {Returns }(s) \\leftarrow \\text { an empty list, for all } s \\in \\mathcal{S}} \\\\ {N(s) \\text { Visit counter for all } s \\in \\mathcal{S}}\\end{array}}\\end{array}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{array}{l}{\\text { Loop forever (for each episode): }} \\\\ {\\begin{array}{l}{\\qquad \\text {Generate an episode following } \\pi : S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, \\ldots, S_{T-1}, A_{T-1}, R_{T}} \\\\ {\\qquad G \\leftarrow 0} \\\\ {\\qquad \\text {Loop for each step of episode, } t=T-1, T-2, \\ldots, 0 :} \\\\ {\\qquad \\qquad {G \\leftarrow \\gamma G+R_{t+1}} \\\\ {\\qquad \\qquad \\text {Unless } S_{t} \\text { appears in } S_{0}, S_{1}, \\ldots, S_{t-1} :} \\\\ {\\qquad \\qquad \\qquad N(s) \\leftarrow N(s) + 1 \\text { Increment the state count }} \\\\  {\\qquad \\qquad \\qquad V\\left(S_{t}\\right) \\leftarrow V(S_t) + \\frac{(G - V(S))}{N(s)}}}\\end{array}}\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2) (Programming) Using first-vist MC PE, write the pseudocode for the policy iteration before writing in Python. Apply the incremental version to OpenAI Gym BlackJack-V0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{l}{\\text { Initialize: }} \\\\ {\\qquad \\begin{array}{l}{\\pi(s) \\in \\mathcal{A}(s) \\text { (arbitrarily), for all } s \\in \\mathcal{S}} \\\\ {Q(s, a) \\in \\mathbb{R} \\text { (arbitrarily), for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)} \\\\ {\\text {Returns }(s, a) \\leftarrow \\text { empty list, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)}\\end{array}}\\end{array}\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\n",
    "\\begin{array}{l}{\\text { Loop forever (for each episode): }} \\\\ {\\qquad \\begin{array}{l}{\\text {Choose } S_{0} \\in \\mathcal{S}, A_{0} \\in \\mathcal{A}\\left(S_{0}\\right) \\text { randomly such that all pairs have probability }>0} \\\\ {\\text {Generate an episode from } S_{0}, A_{0}, \\text { following } \\pi : S_{0}, A_{0}, R_{1}, \\ldots, S_{T-1}, A_{T-1}, R_{T}} \\\\ { G \\leftarrow 0} \\\\ {\\text { Loop for each step of episode, } t=T-1, T-2, \\ldots, 0 :} \\\\ {\\qquad G \\leftarrow \\gamma G+R_{t+1}} \\\\ {\\qquad \\text {Unless the pair } S_{t}, A_{t} \\text { appears in } S_{0}, A_{0}, S_{1}, A_{1} \\ldots, S_{t-1}, A_{t-1} :} \\\\ {\\qquad \\text{Append $G$ to $Returns(S_t, A_t)$}} \\\\ {\\qquad Q(S_t, A_t) \\leftarrow average(Returns(S_t, A_t))} \\\\ {\\qquad \\pi(S_t) \\leftarrow argmax_aQ(S_t, a)} \\end{array}}\\end{array}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Visit MC Policy Iteration (Incremental Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=0.99, epsilon=0.1):\n",
    "    \n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(100):\n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Find all (state, action) pairs we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
    "                                       if x[0] == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = Q[state][action] + (G - Q[state][action]) / returns_count[sa_pair]\n",
    "        \n",
    "        # The policy is improved implicitly by changing the Q dictionary\n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50000/50000."
     ]
    }
   ],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=50000, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {(16, 6, False): -0.04545454545454548,\n",
       "             (15, 5, True): -0.06577962711864413,\n",
       "             (18, 5, True): 0.12307692307692306,\n",
       "             (19, 9, True): 0.5205479452054795,\n",
       "             (20, 4, False): 0.6653061224489792,\n",
       "             (19, 1, False): -0.07222222222222222,\n",
       "             (14, 9, False): -0.37322322738386327,\n",
       "             (20, 8, True): 0.7272727272727271,\n",
       "             (19, 5, False): 0.482658959537572,\n",
       "             (21, 7, True): 0.9349999999999999,\n",
       "             (13, 10, False): -0.4025573378839591,\n",
       "             (17, 2, True): -0.16499999999999998,\n",
       "             (17, 10, False): -0.4664931685100844,\n",
       "             (20, 10, False): 0.4251851851851854,\n",
       "             (21, 1, True): 0.6210045662100452,\n",
       "             (12, 3, False): -0.2740822523364484,\n",
       "             (18, 7, False): 0.4447439353099732,\n",
       "             (11, 3, False): 0.11270345320197046,\n",
       "             (17, 5, False): -0.0361111111111111,\n",
       "             (12, 10, False): -0.4251020541381465,\n",
       "             (20, 3, False): 0.6574225122349103,\n",
       "             (6, 8, False): -0.31444285714285714,\n",
       "             (15, 2, True): -0.018540000000000022,\n",
       "             (12, 2, False): -0.37655860349127196,\n",
       "             (11, 4, False): 0.29969076923076904,\n",
       "             (5, 10, False): -0.5128205128205129,\n",
       "             (19, 10, False): -0.04142441860465127,\n",
       "             (16, 10, False): -0.5553607447633826,\n",
       "             (20, 1, True): -0.3757344524999999,\n",
       "             (10, 1, False): -0.7216494845360826,\n",
       "             (11, 10, False): 0.04697412180974477,\n",
       "             (15, 10, False): -0.5907710989678205,\n",
       "             (14, 2, False): -0.26455026455026465,\n",
       "             (21, 10, True): 0.9094076655052267,\n",
       "             (15, 8, False): -0.5249266862170086,\n",
       "             (15, 5, False): -0.15555555555555559,\n",
       "             (14, 10, False): -0.5178873472949393,\n",
       "             (21, 3, True): 0.8837209302325583,\n",
       "             (15, 3, False): -0.2346723044397464,\n",
       "             (18, 3, True): 0.06557377049180327,\n",
       "             (19, 2, False): 0.4440154440154441,\n",
       "             (18, 10, False): -0.20590207914151543,\n",
       "             (16, 7, False): -0.41725641025641047,\n",
       "             (13, 7, False): -0.455470737913486,\n",
       "             (13, 8, False): -0.5037593984962407,\n",
       "             (15, 4, True): -0.1111111111111111,\n",
       "             (21, 8, True): 0.9406392694063929,\n",
       "             (9, 10, False): -0.2233035577087035,\n",
       "             (17, 9, False): -0.42506142506142486,\n",
       "             (12, 7, False): -0.44907407407407424,\n",
       "             (10, 10, False): -0.09564439915966391,\n",
       "             (12, 4, False): -0.2343598522167486,\n",
       "             (15, 6, False): -0.414245810055866,\n",
       "             (17, 3, False): -0.4903225806451615,\n",
       "             (19, 3, False): 0.42032967032967034,\n",
       "             (18, 5, False): 0.2242744063324539,\n",
       "             (13, 3, False): -0.4065764845605702,\n",
       "             (18, 7, True): 0.3615403846153846,\n",
       "             (16, 5, False): -0.13013698630136983,\n",
       "             (14, 6, False): -0.062176165803108786,\n",
       "             (17, 6, False): -0.05181347150259067,\n",
       "             (21, 5, False): 0.8794326241134749,\n",
       "             (17, 8, True): -0.17769230769230773,\n",
       "             (16, 9, False): -0.555555555555555,\n",
       "             (20, 7, False): 0.7674858223062384,\n",
       "             (15, 2, False): -0.220708446866485,\n",
       "             (8, 7, False): 0.11073629032258064,\n",
       "             (13, 1, False): -0.6344043062200959,\n",
       "             (8, 6, False): 0.009345794392523367,\n",
       "             (10, 4, False): 0.28023294797687875,\n",
       "             (7, 10, False): -0.5531335149863762,\n",
       "             (19, 4, False): 0.32013201320132023,\n",
       "             (6, 10, False): -0.33391431131868154,\n",
       "             (19, 9, False): 0.2761627906976743,\n",
       "             (21, 9, True): 0.9436619718309859,\n",
       "             (20, 1, False): 0.17095588235294115,\n",
       "             (13, 2, False): -0.2559241706161136,\n",
       "             (17, 4, False): -0.12312312312312301,\n",
       "             (17, 1, False): -0.619512195121951,\n",
       "             (14, 3, False): -0.3699702070707071,\n",
       "             (17, 7, False): -0.11924119241192414,\n",
       "             (15, 10, True): -0.3254154750000001,\n",
       "             (20, 8, False): 0.8115299334811528,\n",
       "             (12, 5, False): -0.24661328320801995,\n",
       "             (16, 3, False): -0.5514824719101117,\n",
       "             (18, 1, False): -0.4252336448598131,\n",
       "             (12, 9, False): -0.323332634032634,\n",
       "             (8, 10, False): -0.2834559734345351,\n",
       "             (9, 6, False): 0.15286071428571438,\n",
       "             (10, 9, False): 0.10766942487046631,\n",
       "             (20, 5, False): 0.6463414634146339,\n",
       "             (9, 9, False): -0.006785547169811323,\n",
       "             (18, 4, False): 0.08517350157728705,\n",
       "             (9, 1, False): -0.4107536510738255,\n",
       "             (21, 2, True): 0.8851063829787232,\n",
       "             (18, 9, False): -0.2197802197802197,\n",
       "             (14, 4, False): -0.19095477386934656,\n",
       "             (18, 2, True): 0.3768115942028985,\n",
       "             (15, 7, False): -0.4404145077720207,\n",
       "             (18, 6, False): 0.20454545454545447,\n",
       "             (21, 3, False): 0.8680851063829783,\n",
       "             (14, 7, False): -0.31537665094339595,\n",
       "             (16, 1, False): -0.6096256684491976,\n",
       "             (10, 6, False): 0.20807426315789473,\n",
       "             (13, 10, True): -0.15957624993749997,\n",
       "             (21, 10, False): 0.8776859504132227,\n",
       "             (4, 6, False): -0.0014072142857142894,\n",
       "             (19, 7, True): 0.6944444444444442,\n",
       "             (16, 4, False): -0.24423963133640536,\n",
       "             (20, 6, False): 0.6843220338983051,\n",
       "             (15, 9, False): -0.4295758928571427,\n",
       "             (14, 5, False): -0.2511239608801957,\n",
       "             (13, 4, False): -0.28533333333333355,\n",
       "             (13, 6, False): -0.15789473684210528,\n",
       "             (15, 1, True): -0.5281120862068965,\n",
       "             (11, 8, False): 0.21437834101382494,\n",
       "             (20, 3, True): 0.58,\n",
       "             (13, 9, False): -0.5330188679245286,\n",
       "             (5, 1, False): -0.4546867783333333,\n",
       "             (20, 2, True): 0.7066666666666666,\n",
       "             (21, 4, False): 0.9019607843137254,\n",
       "             (17, 10, True): -0.25724409448818897,\n",
       "             (14, 8, True): -0.15704775000000004,\n",
       "             (12, 6, False): -0.21291955922865016,\n",
       "             (9, 3, False): -0.21052631578947364,\n",
       "             (9, 2, False): -0.11118461538461535,\n",
       "             (15, 1, False): -0.6310806896551723,\n",
       "             (17, 8, False): -0.46045197740112986,\n",
       "             (19, 8, False): 0.4734848484848483,\n",
       "             (21, 9, False): 0.9719101123595506,\n",
       "             (18, 2, False): 0.12996389891696747,\n",
       "             (6, 5, False): -0.15395865517241383,\n",
       "             (20, 5, True): 0.6338028169014082,\n",
       "             (17, 2, False): -0.1698630136986302,\n",
       "             (11, 7, False): 0.1699116279069767,\n",
       "             (13, 6, True): -0.088875,\n",
       "             (19, 8, True): 0.5172413793103449,\n",
       "             (15, 4, False): -0.22580645161290336,\n",
       "             (14, 3, True): -0.17647058823529416,\n",
       "             (7, 4, False): -0.05255636170212764,\n",
       "             (8, 8, False): -0.18709493478260886,\n",
       "             (18, 8, False): 0.07719298245614027,\n",
       "             (20, 10, True): 0.492592592592592,\n",
       "             (11, 5, False): 0.29952307211538487,\n",
       "             (18, 3, False): 0.08620689655172416,\n",
       "             (7, 3, False): -0.2789510728124999,\n",
       "             (20, 2, False): 0.6077097505668938,\n",
       "             (19, 6, False): 0.47882736156351746,\n",
       "             (14, 8, False): -0.5087719298245615,\n",
       "             (21, 4, True): 0.9195979899497485,\n",
       "             (14, 10, True): -0.20911951960893857,\n",
       "             (10, 7, False): 0.28838957812500027,\n",
       "             (16, 10, True): -0.4057389296756757,\n",
       "             (21, 6, True): 0.9162561576354681,\n",
       "             (12, 1, False): -0.5288629419642858,\n",
       "             (8, 3, False): -0.06907053488372093,\n",
       "             (18, 10, True): -0.19157088122605367,\n",
       "             (6, 6, False): -0.030764630769230736,\n",
       "             (14, 1, False): -0.49820943396226425,\n",
       "             (9, 7, False): 0.05136428571428573,\n",
       "             (14, 1, True): -0.4274303478260868,\n",
       "             (5, 9, False): -0.4107268536585365,\n",
       "             (17, 5, True): -0.03508771929824561,\n",
       "             (16, 5, True): -0.08963819999999999,\n",
       "             (13, 8, True): -0.11549613705882349,\n",
       "             (13, 5, False): -0.16616314199395765,\n",
       "             (8, 4, False): 0.11384999999999998,\n",
       "             (20, 6, True): 0.7200000000000002,\n",
       "             (21, 5, True): 0.9411764705882355,\n",
       "             (17, 3, True): 0.08620689655172416,\n",
       "             (10, 2, False): 0.2535969512195121,\n",
       "             (19, 7, False): 0.6384839650145772,\n",
       "             (12, 8, False): -0.48129675810473804,\n",
       "             (16, 6, True): 0.11159999999999996,\n",
       "             (7, 8, False): -0.46329893617021267,\n",
       "             (6, 9, False): -0.14703455555555553,\n",
       "             (15, 8, True): -0.3354254237288136,\n",
       "             (11, 1, False): -0.07857508860759489,\n",
       "             (9, 5, False): 0.22225116279069765,\n",
       "             (20, 9, False): 0.7647058823529422,\n",
       "             (7, 9, False): -0.2898000000000001,\n",
       "             (19, 10, True): -0.04318936877076408,\n",
       "             (11, 2, False): 0.22042058823529417,\n",
       "             (19, 5, True): -0.00028695652173914635,\n",
       "             (11, 9, False): 0.1658373157894737,\n",
       "             (16, 8, False): -0.5511363636363633,\n",
       "             (8, 5, False): -0.05380763358778624,\n",
       "             (10, 5, False): 0.2609840429447852,\n",
       "             (5, 7, False): -0.24309779999999998,\n",
       "             (16, 2, False): -0.29344729344729353,\n",
       "             (14, 4, True): -0.23076923076923073,\n",
       "             (4, 1, False): -0.5900499,\n",
       "             (7, 2, False): -0.03557093023255813,\n",
       "             (12, 10, True): -0.30136522680000005,\n",
       "             (21, 7, False): 0.9538461538461541,\n",
       "             (14, 7, True): 0.12105789473684209,\n",
       "             (6, 4, False): -0.07604850000000003,\n",
       "             (12, 4, True): -0.1111,\n",
       "             (16, 7, True): -0.5384615384615383,\n",
       "             (19, 2, True): 0.4461538461538462,\n",
       "             (21, 1, False): 0.6139534883720933,\n",
       "             (5, 8, False): -0.08822984210526313,\n",
       "             (11, 6, False): 0.2948314550561798,\n",
       "             (15, 6, True): -0.18181818181818177,\n",
       "             (5, 5, False): -0.07471326396226415,\n",
       "             (4, 10, False): -0.30408436928571414,\n",
       "             (13, 9, True): -0.2656073414634147,\n",
       "             (8, 1, False): -0.6747292682926832,\n",
       "             (6, 1, False): -0.5531835205479452,\n",
       "             (14, 6, True): -0.14285714285714288,\n",
       "             (17, 6, True): -0.03788653846153846,\n",
       "             (6, 7, False): -0.17723874193548386,\n",
       "             (9, 8, False): 0.11513835616438356,\n",
       "             (10, 8, False): -0.022890173410404627,\n",
       "             (19, 3, True): 0.41891891891891886,\n",
       "             (7, 1, False): -0.5489024322222221,\n",
       "             (7, 5, False): -0.056179775280898875,\n",
       "             (10, 3, False): 0.0008011026486486553,\n",
       "             (17, 7, True): -0.22413793103448282,\n",
       "             (16, 1, True): -0.35304107142857155,\n",
       "             (18, 8, True): 0.04477611940298509,\n",
       "             (18, 1, True): -0.3350661971830985,\n",
       "             (16, 2, True): -0.09880200000000003,\n",
       "             (19, 1, True): -0.160919540229885,\n",
       "             (4, 8, False): 0.055905882352941166,\n",
       "             (8, 9, False): -0.22754658778625952,\n",
       "             (12, 9, True): -0.42857142857142855,\n",
       "             (20, 4, True): 0.7702702702702705,\n",
       "             (6, 2, False): -0.08365169999999998,\n",
       "             (18, 6, True): 0.19642857142857148,\n",
       "             (21, 6, False): 0.9009009009009007,\n",
       "             (8, 2, False): 0.09732676056338029,\n",
       "             (16, 4, True): 0.17578,\n",
       "             (9, 4, False): 0.02180977443609024,\n",
       "             (18, 9, True): 0.05263157894736842,\n",
       "             (5, 4, False): -0.2653061224489797,\n",
       "             (12, 7, True): -0.04713814285714286,\n",
       "             (5, 3, False): -0.054088815000000026,\n",
       "             (14, 2, True): 0.2624644,\n",
       "             (4, 5, False): -0.0009045000000000164,\n",
       "             (15, 3, True): 0.06410655737704916,\n",
       "             (21, 8, False): 0.9220779220779219,\n",
       "             (4, 2, False): 0.06666666666666667,\n",
       "             (21, 2, False): 0.9,\n",
       "             (13, 5, True): -0.028565742857142824,\n",
       "             (7, 6, False): 0.010581620689655164,\n",
       "             (16, 8, True): 0.03845769230769226,\n",
       "             (18, 4, True): 0.10977999999999997,\n",
       "             (12, 8, True): -0.13266660000000002,\n",
       "             (7, 7, False): -0.2010044831460675,\n",
       "             (17, 4, True): 0.0888888888888889,\n",
       "             (13, 1, True): -0.352351103076923,\n",
       "             (5, 6, False): -0.09803921568627452,\n",
       "             (19, 4, True): 0.5573770491803279,\n",
       "             (13, 4, True): 0.2940510638297872,\n",
       "             (12, 2, True): 0.2450579175,\n",
       "             (4, 9, False): -0.24780318750000005,\n",
       "             (15, 7, True): -0.21779999999999997,\n",
       "             (19, 6, True): 0.25,\n",
       "             (12, 5, True): 0.6363636363636362,\n",
       "             (20, 9, True): 0.7272727272727274,\n",
       "             (14, 9, True): 0.0008446595744680922,\n",
       "             (6, 3, False): -0.2691399715714285,\n",
       "             (16, 9, True): -0.5408999999999999,\n",
       "             (5, 2, False): -0.15555555555555556,\n",
       "             (12, 3, True): -0.0017174347826087177,\n",
       "             (16, 3, True): -0.19909559999999998,\n",
       "             (14, 5, True): -0.24324324324324326,\n",
       "             (15, 9, True): -0.04742790697674415,\n",
       "             (13, 2, True): 0.03491226315789472,\n",
       "             (13, 7, True): -0.2380952380952381,\n",
       "             (17, 1, True): -0.5709607820296876,\n",
       "             (17, 9, True): -0.29268292682926833,\n",
       "             (20, 7, True): 0.7530864197530863,\n",
       "             (12, 1, True): -0.7100664999999998,\n",
       "             (13, 3, True): -0.023170499999999986,\n",
       "             (4, 3, False): 0.1542471584210527,\n",
       "             (12, 6, True): 0.217261,\n",
       "             (4, 4, False): -0.7333333333333334,\n",
       "             (4, 7, False): -0.0029650500000000385})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the policy obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW GAME--------------\n",
      "[0.995 0.005] (20, 7, False) 1.0 (20, 7, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (19, 10, False) 1.0 (19, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (13, 5, False) -1 (23, 5, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (6, 8, False) 0 (10, 8, False) False\n",
      "[0.005 0.995] (10, 8, False) 0 (19, 8, False) False\n",
      "[0.995 0.005] (19, 8, False) -1.0 (19, 8, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (21, 8, True) 1.0 (21, 8, True) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (11, 10, False) 0 (13, 10, False) False\n",
      "[0.005 0.995] (13, 10, False) -1 (22, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (21, 1, True) 0.0 (21, 1, True) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (8, 7, False) 1.0 (8, 7, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (7, 4, False) 1.0 (7, 4, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (13, 10, False) 0 (15, 10, False) False\n",
      "[0.995 0.005] (15, 10, False) -1.0 (15, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (12, 10, False) 0 (20, 10, False) False\n",
      "[0.995 0.005] (20, 10, False) 0.0 (20, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (19, 1, False) -1.0 (19, 1, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (13, 10, False) 0 (15, 10, False) False\n",
      "[0.995 0.005] (15, 10, False) -1.0 (15, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (17, 2, False) -1.0 (17, 2, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (10, 6, False) 0 (13, 6, False) False\n",
      "[0.995 0.005] (13, 6, False) -1.0 (13, 6, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (9, 5, False) -1.0 (9, 5, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (20, 2, False) 1.0 (20, 2, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (17, 10, False) -1.0 (17, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (12, 7, False) -1 (22, 7, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (10, 3, False) 0 (14, 3, False) False\n",
      "[0.995 0.005] (14, 3, False) -1.0 (14, 3, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (15, 8, False) -1 (25, 8, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (17, 4, False) 1.0 (17, 4, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (12, 8, False) -1 (22, 8, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (11, 10, False) 0 (20, 10, False) False\n",
      "[0.995 0.005] (20, 10, False) 0.0 (20, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (20, 9, False) 1.0 (20, 9, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (19, 3, False) 1.0 (19, 3, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (13, 10, False) -1 (23, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (12, 3, False) -1.0 (12, 3, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (14, 10, False) -1.0 (14, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (12, 6, True) 0 (14, 6, True) False\n",
      "[0.005 0.995] (14, 6, True) 0 (19, 6, True) False\n",
      "[0.005 0.995] (19, 6, True) 0 (19, 6, False) False\n",
      "[0.995 0.005] (19, 6, False) 1.0 (19, 6, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (14, 8, False) 1.0 (14, 8, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (19, 10, False) 0.0 (19, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (16, 10, False) -1 (26, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (13, 6, False) -1.0 (13, 6, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (15, 10, False) -1.0 (15, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (20, 7, False) 1.0 (20, 7, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (14, 10, False) -1.0 (14, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (20, 2, False) -1.0 (20, 2, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (13, 5, False) -1 (23, 5, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (15, 6, False) -1 (24, 6, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (19, 10, False) -1.0 (19, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (13, 2, False) 1.0 (13, 2, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (14, 10, False) -1.0 (14, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (20, 10, False) 0.0 (20, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (21, 9, True) 1.0 (21, 9, True) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (10, 10, False) 0 (20, 10, False) False\n",
      "[0.995 0.005] (20, 10, False) 1.0 (20, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (11, 10, False) 0 (12, 10, False) False\n",
      "[0.005 0.995] (12, 10, False) -1 (22, 10, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (21, 2, True) 1.0 (21, 2, True) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.005 0.995] (11, 2, False) 0 (16, 2, False) False\n",
      "[0.995 0.005] (16, 2, False) -1.0 (16, 2, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "NEW GAME--------------\n",
      "[0.995 0.005] (18, 7, False) 1.0 (18, 7, False) True\n",
      "DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "countwins = 0\n",
    "num_episodes=10\n",
    "for i_episode in range(1, num_episodes + 1):\n",
    "    # Print out which episode we're on, useful for debugging.\n",
    "    if i_episode % 1000 == 0:\n",
    "        print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Generate an episode.\n",
    "    # An episode is an array of (state, action, reward) tuples\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    print('NEW GAME--------------')\n",
    "\n",
    "    for t in range(100):\n",
    "        probs = policy(state)\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        print(probs, state, reward, next_state, done)\n",
    "        episode.append((state, action, reward))\n",
    "        if done:\n",
    "            print('DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            if reward == 1:\n",
    "                countwins += reward\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countwins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. (Programming) Implement Policy Iteration (DP) and compare its performance with the incremental first-visit MC policy iteration that you have implemented in 7 to solve the following GridWorld environment.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maze Problem (Practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from operator import add\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Grid Environment File\n",
    "\n",
    "Simple text file with three characters, 'O', 'H', and 'G'.\n",
    "- 'O': open space\n",
    "- 'H': Wall or obstacles\n",
    "- 'G': Goal location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOOHOOOOO\n",
      "OOOHOOHOO\n",
      "OOOOOOHOO\n",
      "OOOOHHHOO\n",
      "OOHOOOOOH\n",
      "OOHOOOGOO\n",
      "OOOOOOOOO\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cat grid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze example\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\" Grid World environment\n",
    "            there are four actions (left, right, up, and down) to move an agent\n",
    "            In a grid, if it reaches a goal, it get 30 points of reward.\n",
    "            If it falls in a hole or moves out of the grid world, it gets -5.\n",
    "            Each step costs -1 point.\n",
    "\n",
    "        to test GridWorld, run the following sample codes:\n",
    "\n",
    "            env = GridWorld('grid.txt')\n",
    "\n",
    "            env.print_map()\n",
    "            print [2,3], env.check_state([2,3])\n",
    "            print [0,0], env.check_state([0,0])\n",
    "            print [3,4], env.check_state([3,4])\n",
    "            print [10,3], env.check_state([10,3])\n",
    "\n",
    "            env.init([0,0])\n",
    "            print env.next(1)  # right\n",
    "            print env.next(3)  # down\n",
    "            print env.next(0)  # left\n",
    "            print env.next(2)  # up\n",
    "            print env.next(2)  # up\n",
    "\n",
    "        Parameters\n",
    "        ==========\n",
    "        _map        ndarray\n",
    "                    string array read from a file input\n",
    "        _size       1d array\n",
    "                    the size of _map in ndarray\n",
    "        goal_pos    tuple\n",
    "                    the index for the goal location\n",
    "        _actions    list\n",
    "                    list of actions for 4 actions\n",
    "        _s          1d array\n",
    "                    current state\n",
    "    \"\"\"\n",
    "    def __init__(self, fn):\n",
    "        # read a map from a file\n",
    "        self._map = self.read_map(fn)\n",
    "        self._size = np.asarray(self._map.shape)\n",
    "        self.goal_pos = np.where(self._map == 'G')\n",
    "\n",
    "        # definition of actions (left, right, up, and down repectively)\n",
    "        self._actions = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self._s = None\n",
    "\n",
    "    def get_cur_state(self):\n",
    "        return self._s\n",
    "\n",
    "    def get_size(self):\n",
    "        return self._size\n",
    "\n",
    "    def read_map(self, fn):\n",
    "        grid = []\n",
    "        with open(fn) as f:\n",
    "            for line in f:\n",
    "                grid.append(list(line.strip()))\n",
    "        return np.asarray(grid)\n",
    "\n",
    "    def print_map(self):\n",
    "        print( self._map )\n",
    "\n",
    "    def check_state(self, s):\n",
    "        #print('c:',s)\n",
    "        if isinstance(s, collections.Iterable) and len(s) == 2:\n",
    "            #print('HI', isinstance(s, collections.Iterable))\n",
    "            if s[0] < 0 or s[1] < 0 or\\\n",
    "               s[0] >= self._size[0] or s[1] >= self._size[1]:\n",
    "                #print(\"NOT POSSIBLE\")\n",
    "                return 'N'\n",
    "            return self._map[tuple(s)].upper()\n",
    "        else:\n",
    "            return 'F'  # wrong input\n",
    "\n",
    "    def init(self, state=None):\n",
    "        if state is None:\n",
    "            #print(\"HI\")\n",
    "            s = [0, 0]\n",
    "        else:\n",
    "            s = state\n",
    "\n",
    "        if self.check_state(s) == 'O':\n",
    "            self._s = np.asarray(state)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state for init\")\n",
    "\n",
    "    def next(self, a):\n",
    "        s1 = list(map(add, self._s, self._actions[a]))# CHANGE IN CODE - BUG FIX\n",
    "        # state transition\n",
    "        curr = self.check_state(s1)\n",
    "        #print(s1, 'IN NEXT')\n",
    "        if curr == 'H' or curr == 'N':\n",
    "            return -5\n",
    "        elif curr == 'F':\n",
    "            warnings.warn(\"invalid state \" + str(s1))\n",
    "            return -5\n",
    "        elif curr == 'G':\n",
    "            self._s = s1\n",
    "            return 30\n",
    "        else:\n",
    "            self._s = s1\n",
    "            return -1\n",
    "        \n",
    "    def is_goal(self):\n",
    "        return self.check_state(self._s) == 'G'\n",
    "           \n",
    "    def get_actions(self):\n",
    "        return self._actions\n",
    "    \n",
    "    # Added a helper function in the environment to generate list of states\n",
    "    def get_state_list(self):\n",
    "        \n",
    "        self.statelist = []\n",
    "        for i in range(self._size[0]):\n",
    "            for j in range(self._size[1]):\n",
    "                self.statelist.append([i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' 'O' 'O' 'H' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'H' 'O' 'O' 'H' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'H' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'H' 'H' 'H' 'O' 'O']\n",
      " ['O' 'O' 'H' 'O' 'O' 'O' 'O' 'O' 'H']\n",
      " ['O' 'O' 'H' 'O' 'O' 'O' 'G' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']]\n",
      "[7 9]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(\"grid.txt\")\n",
    "env.print_map()\n",
    "env.get_state_list()\n",
    "print(env._size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-left to (0,0)\n",
    "def coord_convert(s, sz):\n",
    "    return [s[1], sz[0]-s[0]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP Based Policy Iteration\n",
    "\n",
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.01):\n",
    "    \n",
    "    # Start with a random (all 0) value function\n",
    "    total_states = env._size[0] * env._size[1]\n",
    "    V = np.zeros([env._size[0], env._size[1]])\n",
    "    ct = 0\n",
    "    deltalist = [0]\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in env.statelist:\n",
    "            #env._s = s#use unit\n",
    "            #env.init()\n",
    "            v = 0\n",
    "            for action, action_prob in enumerate(policy[env.statelist.index(s)]):\n",
    "                # For each action, look at the possible next states...\n",
    "                env._s = s\n",
    "                reward = env.next(action)\n",
    "                next_state = env.get_cur_state()\n",
    "                prob = 1\n",
    "                # Calculate the expected value\n",
    "                v += action_prob * prob * (reward + (discount_factor * V[next_state[0], next_state[1]]))\n",
    "                \n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s[0],s[1]]))\n",
    "            \n",
    "            V[s[0],s[1]] = v\n",
    "        deltalist.append(delta)\n",
    "        ct=ct+1\n",
    "        # convergence condition\n",
    "        if np.abs(deltalist[-1] - deltalist[-2]) < theta:\n",
    "            break \n",
    "    #print(ct)\n",
    "    return np.array(V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([(env._size[0] * env._size[1]), len(env._actions)]) / len(env._actions)\n",
    "    rewardlistimprove = []\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        # EVALUATE POLICY\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        #print('Value:', V)\n",
    "        # Will be set to false if there are any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in env.statelist:\n",
    "            # Choose the best action under the currect policy\n",
    "            chosen_a = np.argmax(policy[env.statelist.index(s)])\n",
    "            #print(chosen_a)\n",
    "            A = np.zeros(len(env._actions))\n",
    "            for action, action_prob in enumerate(policy[env.statelist.index(s)]):\n",
    "                env._s = s\n",
    "                #print(policy[env.statelist.index(s)])\n",
    "                prob = 1\n",
    "                reward = env.next(action)\n",
    "                next_state = env.get_cur_state()\n",
    "                #print(reward, next_state,  V[next_state[0], next_state[1]])\n",
    "                A[action] += prob * (reward + discount_factor * V[next_state[0], next_state[1]])\n",
    "                \n",
    "            best_a = np.argmax(A)\n",
    "            \n",
    "            # Update policy greedily\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "                \n",
    "            policy[env.statelist.index(s)] = np.eye(len(env._actions))[best_a]\n",
    "\n",
    "        # Lets gather the reward from the improved policy\n",
    "        episode = []\n",
    "        env.init([0,0])\n",
    "        state = list(env.get_cur_state())\n",
    "        #print('Starting State:', state)\n",
    "        rewardsum = 0\n",
    "        #print(env.statelist)\n",
    "        for t in range(100):\n",
    "            probs = policy[env.statelist.index(state)]\n",
    "            action = np.argmax(probs)\n",
    "            reward = env.next(action)\n",
    "            next_state = list(env.get_cur_state())\n",
    "            #env._s = next_state\n",
    "            rewardsum += reward\n",
    "            episode.append((state, action, reward))\n",
    "            #print(episode[t], next_state, probs)\n",
    "            if env.is_goal():\n",
    "                #print(\"Reached Goal\")\n",
    "                break\n",
    "            state = list.copy(next_state)\n",
    "        \n",
    "        rewardlistimprove.append(rewardsum)\n",
    "        iterations+=1\n",
    "        # Return optimal policy, optimal state values and reward list\n",
    "        if policy_stable:\n",
    "            return policy, V, rewardlistimprove, iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy, V, rewardlistimprove, iterations = policy_improvement(env, discount_factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 46.70906424  53.04819298  60.08785719  53.07907147  60.11564783\n",
      "   67.93128695  76.61282925  86.25673633  76.6310627 ]\n",
      " [ 53.04819298  60.08785719  67.90627537  76.59031883  67.93128695\n",
      "   60.13815825  86.25673633  96.9701627   86.27314643]\n",
      " [ 60.08785719  67.90627537  76.59031883  86.23647695  76.61282925\n",
      "   67.95154633  96.9701627  108.87214643  96.98493178]\n",
      " [ 67.90627537  76.59031883  86.23647695  96.95192925 108.85573633\n",
      "  122.0801627  136.77214643 122.09493178 108.88543861]\n",
      " [ 60.11564783  67.93128695  96.95192925 108.85573633 122.0801627\n",
      "  136.77214643 153.09493178 136.78543861 122.10689475]\n",
      " [ 67.93128695  76.61282925 108.85573633 122.0801627  136.77214643\n",
      "  153.09493178 136.78543861 153.10689475 136.79620527]\n",
      " [ 76.61282925  86.25673633  96.9701627  108.87214643 122.09493178\n",
      "  136.78543861 153.10689475 136.79620527 122.11658474]] \n",
      " [[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] \n",
      " [-100, -100, -100, -100, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "print(np.reshape(V, [env._size[0], env._size[1]]), '\\n',  policy, '\\n', rewardlistimprove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check agent's improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f72b59e8cf8>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(iterations)\n",
    "plt.plot(x, rewardlistimprove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DP based Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting State: [0, 0]\n",
      "([0, 0], 1, -1) [0, 1] [0. 1. 0. 0.]\n",
      "([0, 1], 1, -1) [0, 2] [0. 1. 0. 0.]\n",
      "([0, 2], 3, -1) [1, 2] [0. 0. 0. 1.]\n",
      "([1, 2], 3, -1) [2, 2] [0. 0. 0. 1.]\n",
      "([2, 2], 1, -1) [2, 3] [0. 1. 0. 0.]\n",
      "([2, 3], 3, -1) [3, 3] [0. 0. 0. 1.]\n",
      "([3, 3], 3, -1) [4, 3] [0. 0. 0. 1.]\n",
      "([4, 3], 1, -1) [4, 4] [0. 1. 0. 0.]\n",
      "([4, 4], 1, -1) [4, 5] [0. 1. 0. 0.]\n",
      "([4, 5], 1, -1) [4, 6] [0. 1. 0. 0.]\n",
      "([4, 6], 3, 30) [5, 6] [0. 0. 0. 1.]\n",
      "Hurray\n"
     ]
    }
   ],
   "source": [
    "episode = []\n",
    "env.init([0,0])\n",
    "state = list(env.get_cur_state())\n",
    "print('Starting State:', state)\n",
    "rewardlist = []\n",
    "#print(env.statelist)\n",
    "for t in range(100):\n",
    "    probs = policy[env.statelist.index(state)]\n",
    "    action = np.argmax(probs)\n",
    "    reward = env.next(action)\n",
    "    next_state = list(env.get_cur_state())\n",
    "    #env._s = next_state\n",
    "    rewardlist.append(reward)\n",
    "    episode.append((state, action, reward))\n",
    "    print(episode[t], next_state, probs)\n",
    "    if env.is_goal():\n",
    "        print(\"Hurray\")\n",
    "        break\n",
    "    state = list.copy(next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Based Policy Iteration on Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        #print(observation, 'de')\n",
    "        best_action = np.argmax(Q[tuple(observation)])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_grid_world(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(len(env._actions)))\n",
    "    rewardlist = []\n",
    "    # The policy we're following\n",
    "    policy = epsilon_greedy_policy(Q, epsilon, len(env._actions))\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        env.init([0, 0])\n",
    "        state = env.get_cur_state()\n",
    "        rewardsum = 0\n",
    "\n",
    "        for t in range(100):\n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            reward = env.next(action)\n",
    "            next_state = env.get_cur_state()\n",
    "            rewardsum += reward\n",
    "            episode.append((state, action, reward))\n",
    "            if env.is_goal():\n",
    "                break\n",
    "            state = next_state\n",
    "        rewardlist.append(rewardsum)\n",
    "        #print(episode)\n",
    "        # Find all (state, action) pairs we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
    "        \n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            #print(state)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode) if tuple(x[0]) == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = Q[state][action] + (G - Q[state][action]) / returns_count[sa_pair]\n",
    "        \n",
    "        # The policy is improved implicitly by changing the Q dictionary\n",
    "    \n",
    "    return Q, policy, rewardlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000/5000."
     ]
    }
   ],
   "source": [
    "Q, policy, rewardlist = mc_control_grid_world(env, num_episodes=5000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check agent's progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f72b5a87780>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(5000)\n",
    "plt.plot(x, rewardlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {(0, 0): 9.771195310864556,\n",
       "             (0, 1): -41.8700209643606,\n",
       "             (1, 0): 12.855627705627702,\n",
       "             (2, 0): -37.229681978798595,\n",
       "             (2, 1): -7.752345215759852,\n",
       "             (3, 0): -60.984848484848484,\n",
       "             (0, 2): -11.523560209424087,\n",
       "             (1, 1): 16.419781604367895,\n",
       "             (1, 2): 19.396126011068507,\n",
       "             (3, 1): -55.6844919786096,\n",
       "             (4, 1): -56.33802816901409,\n",
       "             (4, 0): -61.84210526315789,\n",
       "             (2, 2): 21.669892473118228,\n",
       "             (3, 2): -3.8375796178344004,\n",
       "             (5, 0): -82.4,\n",
       "             (6, 0): -50.0,\n",
       "             (2, 3): 23.6525037936267,\n",
       "             (2, 4): 19.34959349593496,\n",
       "             (3, 3): 25.030099610220883,\n",
       "             (4, 3): 26.320825932504448,\n",
       "             (2, 5): -76.5,\n",
       "             (1, 5): -6.0,\n",
       "             (1, 4): -18.0,\n",
       "             (0, 4): 0.0,\n",
       "             (0, 5): 0.0,\n",
       "             (5, 1): -45.909090909090914,\n",
       "             (6, 1): -53.0,\n",
       "             (6, 2): -32.8,\n",
       "             (6, 3): 26.13114754098359,\n",
       "             (5, 3): 27.57953509571562,\n",
       "             (5, 4): 28.81940332498297,\n",
       "             (4, 4): 27.204347826086956,\n",
       "             (4, 5): 28.8222891566265,\n",
       "             (6, 4): 27.49532710280373,\n",
       "             (5, 5): 30.0,\n",
       "             (6, 5): 28.876106194690266,\n",
       "             (6, 6): 30.0,\n",
       "             (6, 7): 25.0,\n",
       "             (4, 6): 30.0,\n",
       "             (4, 7): 29.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate State Values\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025 0.025 0.025 0.925]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.025 0.025 0.925]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.925 0.025 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.025 0.025 0.925 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n",
      "[0.925 0.025 0.025 0.025]\n"
     ]
    }
   ],
   "source": [
    "#print policy\n",
    "for i in env.statelist:\n",
    "    print(policy(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MC based Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[1, 0]\n",
      "[1, 1]\n",
      "[1, 2]\n",
      "[2, 2]\n",
      "[2, 3]\n",
      "[3, 3]\n",
      "[4, 3]\n",
      "[5, 3]\n",
      "[5, 4]\n",
      "[5, 5]\n"
     ]
    }
   ],
   "source": [
    "episode = []\n",
    "env.init([0, 0])\n",
    "state = env.get_cur_state()\n",
    "rewardlist = []\n",
    "#print(state)\n",
    "for t in range(100):\n",
    "    probs = policy(state)\n",
    "    action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "    #next_state, reward, done, _ = env.step(action)\n",
    "    reward = env.next(action)\n",
    "    print(state)\n",
    "    next_state = env.get_cur_state()\n",
    "    rewardlist.append(reward)\n",
    "    episode.append((state, action, reward))\n",
    "    if env.is_goal():\n",
    "        break\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 30]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
